\chapter{Introduction}
\label{ch:introduction}

\section{Motivation and Background}
\label{sec:motivation}

\lettrine[lines=2,slope=4pt,findent=-3pt]{F}{ace} verification systems powered by deep neural networks have become increasingly deployed in high-stakes decision-making contexts \cite{garvie2016perpetual}, from law enforcement investigations and border security screening to financial services authentication and access control \cite{grother2019frvt}. These systems achieve remarkable accuracy, with state-of-the-art models reporting error rates below 0.1\% on benchmark datasets \cite{deng2019arcface,wang2018cosface}. However, this impressive aggregate performance masks significant disparities: the National Institute of Standards and Technology's Face Recognition Vendor Test (FRVT) found that across 189 commercial algorithms, false positive rates for Asian and African American faces were 10--100 times higher than for Caucasian faces \cite{grother2019frvt}. Similarly, Buolamwini and Gebru's Gender Shades study documented error rates of 20.8\% for dark-skinned females compared to 0.8\% for light-skinned males on commercial face recognition systems \cite{buolamwini2018gender}. Moreover, even when systems achieve low average error rates, the decision-making process of deep neural networks remains fundamentally opaque \cite{lipton2018mythos,castelvecchi2016blackbox}. When a face verification system declares that two face images match---or critically, that they do not match---the internal computational pathway leading to that conclusion is inaccessible to human operators, forensic examiners, and defendants alike \cite{rudin2019stop}. This opacity, combined with documented accuracy disparities across demographic groups \cite{klare2012demographic,phillips2011intro,buolamwini2018gender}, poses severe challenges in contexts where incorrect decisions carry profound consequences, including wrongful criminal charges \cite{hill2020detroit,hill2023pregnant,parks2019wrongful}, denial of essential services, and violations of civil liberties.

The field of Explainable AI (XAI) emerged precisely to address this opacity problem \cite{gunning2017xai,doshivelez2017rigorous}, offering methods to generate human-interpretable explanations for neural network decisions \cite{adadi2018peeking}. Techniques such as Gradient-weighted Class Activation Mapping (Grad-CAM) \cite{selvaraju2019gradcam}, SHapley Additive exPlanations (SHAP) \cite{lundberg2017unified}, and Integrated Gradients (IG) \cite{Sundararajan2017_IG} produce visual saliency maps highlighting which regions of an input image most influenced a model's prediction. These methods have been increasingly adopted in face recognition research, with studies applying them to identify which facial features drive verification decisions \cite{Lin2021_xCos,dhar2021understanding,wang2021survey}.

However, a fundamental problem undermines the utility of these explanations: there exists no reliable method to validate \cite{alvarezmelis2018robustness} whether generated explanations are actually faithful to the model's decision-making process, or merely plausible post-hoc rationalizations that happen to align with human intuitions \cite{rudin2019stop,leavy2020interpretation}. Current evaluation approaches rely on indirect proxy metrics such as insertion and deletion curves, which measure how model confidence changes when highlighted regions are progressively added or removed \cite{petsiuk2018rise,samek2016evaluating}. Yet these metrics have known theoretical flaws: they assume that explanation quality can be inferred from model behavior under distribution shift, they fail to distinguish between multiple competing explanations that yield similar metric scores, and they cannot detect when an explanation method systematically misattributes causality \cite{hooker2019benchmark,Adebayo2018_SanityChecks,tomsett2020sanity}. The XAI community currently lacks a principled framework for falsifying incorrect explanations \cite{Samek2021_XAI_Review,Linardatos2021_XAI_Interpretability,nauta2023quantitative}---for definitively demonstrating that a given explanation misrepresents the model's actual decision logic.

This gap between available explanation methods and rigorous validation mechanisms has profound real-world consequences. The deployment of facial recognition systems in law enforcement has led to multiple documented cases of wrongful arrest, where individuals were detained and charged with serious crimes based solely on algorithmic face matches that were later proven incorrect. Robert Williams, a Black man in Detroit, was arrested on January 9, 2020 and held for 30 hours after a facial recognition system incorrectly matched his driver's license photo to surveillance footage from a shoplifting incident that occurred in October 2018 \cite{hill2020detroit}. Porcha Woodruff, eight months pregnant, was arrested at her home in Detroit on February 16, 2023 for alleged carjacking and robbery based on a false facial recognition match, spending 11 hours in custody before charges were dropped for insufficient evidence \cite{hill2023pregnant}. Similar cases have been documented involving Nijeer Parks in New Jersey, who spent 10 days in jail on a false facial recognition match \cite{parks2019wrongful}.

These documented cases \cite{parks2019wrongful,hill2020detroit,hill2023pregnant} represent not merely technical failures but violations of fundamental rights, where the inability to explain and validate algorithmic decisions directly enabled miscarriages of justice. In each case, the lack of interpretable and validated explanations prevented timely detection of the error \cite{nrc2009strengthening,Leslie2020_BiasFaceRecognition}: investigators had no mechanism to assess whether the algorithmic match was based on genuinely probative facial features or spurious correlations in training data. Law enforcement personnel trusted the system's output without the tools to critically evaluate its reasoning \cite{garvie2016perpetual}.

The urgency of addressing this validation gap extends beyond individual cases to encompass legal, regulatory, and scientific imperatives. The European Union's Artificial Intelligence Act (2024) classifies biometric identification systems as high-risk applications requiring ``transparent and comprehensible'' decision-making processes \cite{euaiact2024}. The General Data Protection Regulation (GDPR) Article 22 grants individuals ``the right not to be subject to a decision based solely on automated processing'' and implicitly requires meaningful explanations for algorithmic decisions \cite{gdpr2016,wachter2017right}. In the United States, forensic science standards increasingly demand scientific validation for evidence presented in court \cite{nrc2009strengthening}. When facial recognition evidence is introduced in criminal proceedings, prosecutors must demonstrate not only that the system is generally accurate but that the specific match in question is reliable and based on scientifically sound reasoning \cite{fed702}.

Without validated explanations, facial recognition evidence fails the Daubert standard for scientific testimony, which requires that methods be testable, peer-reviewed, and have known error rates \cite{daubert1993}. This dissertation argues that current XAI methods cannot meet this standard because they lack falsifiability: there is no experimental protocol to definitively reject an incorrect explanation. This creates an untenable situation where life-altering decisions are made by systems whose reasoning cannot be scientifically validated, defended in court, or meaningfully audited by oversight bodies. The development of falsifiable attribution methods---techniques that generate explanations which can be rigorously tested and potentially disproven---represents not merely a technical contribution but a prerequisite for the responsible deployment of face verification systems in high-stakes contexts. Without such methods, the gap between algorithmic capability and accountability will continue to widen, enabling further injustices while eroding public trust in AI systems.

\section{Problem Statement}
\label{sec:problem_statement}

In face verification systems deployed for forensic analysis, border control, and criminal investigation, explainability is not merely a desirable feature but a legal and ethical necessity \cite{wachter2017right,selbst2018intuitive}. However, current explainable AI (XAI) methods for face verification suffer from a fundamental limitation: they lack falsifiability \cite{popper1959logic}. When Grad-CAM, SHAP, or Integrated Gradients produce a saliency map highlighting specific facial regions as important for a verification decision, there exists no principled method to validate \cite{krishna2022disagreement,zhou2022evaluating} whether these attributions genuinely reflect the model's decision-making process or merely constitute plausible post-hoc narratives. Without ground truth for what features a deep neural network actually uses during inference \cite{doshivelez2017rigorous}, the field operates in an epistemological void where any explanation that appears visually coherent can be defended as faithful.

The inadequacy of current evaluation approaches manifests across multiple dimensions. When applied to the same face verification decision, gradient-based methods like Grad-CAM, perturbation-based methods like SHAP, and path-integration methods like Integrated Gradients frequently produce contradictory saliency maps, highlighting different facial regions with no objective criterion for adjudicating between them \cite{Adebayo2018_SanityChecks,Tomsett2020_SanityChecks,krishna2022disagreement}. Insertion-deletion metrics, widely used to quantify explanation quality, suffer from distribution shift problems: systematically removing or adding pixels creates out-of-distribution samples that elicit unreliable model behavior, undermining the validity of the faithfulness assessment \cite{hooker2019benchmark,rong2022consistent}.

Localization metrics that compare saliency maps to human attention patterns conflate plausibility with faithfulness, assuming that human-interpretable explanations necessarily correspond to model mechanisms \cite{Zhou2022_AttributionCorrectness,jacovi2020faithfully}. Most critically, these evaluation methods provide only relative rankings between explanation methods rather than absolute validation \cite{Nauta2023_QuantitativeEvaluation,hedstrom2023quantus}---they can suggest that Method A produces more faithful explanations than Method B, but cannot establish whether either method achieves ground-truth faithfulness.

This dissertation addresses a significant gap in the explainability literature \cite{Samek2021_XAI_Review}: the absence of falsifiable validation frameworks for attribution methods in embedding-based face verification systems. Prior research on faithfulness evaluation has concentrated almost exclusively on classification tasks \cite{arrieta2020explainable}, where ground truth can sometimes be established through controlled synthetic datasets or known model architectures \cite{Zhou2022_AttributionCorrectness,Adebayo2018_SanityChecks}. Face verification, however, operates in a fundamentally different paradigm---models learn discriminative embedding spaces through metric learning objectives like triplet loss or ArcFace, and decisions emerge from distance computations rather than softmax classification \cite{schroff2015facenet,deng2019arcface,kaya2019metric}.

No existing framework extends counterfactual reasoning \cite{wachter2017counterfactual,Kenny2021_PlausibleCounterfactuals,mothilal2020diverse} to this embedding-space paradigm, leaving a critical gap for high-stakes applications. Furthermore, legal and forensic contexts demand stronger epistemic guarantees than current XAI methods provide. When a face verification system contributes to a criminal conviction or refugee status determination, explaining which facial features drove the decision matters less than validating that the explanation accurately represents the model's reasoning. The European Union's AI Act and similar regulatory frameworks increasingly require not just explanations, but can be interpreted as requiring demonstrably faithful explanations \cite{euaiact2024}---a standard current methods cannot meet.

This research addresses this gap through a counterfactual falsifiability framework specifically designed for face verification systems. The scope focuses on pairwise face verification, where a model receives two face images and produces a similarity score indicating whether they depict the same individual. The proposed approach treats explanation faithfulness as a falsifiable hypothesis: if an attribution method correctly identifies the features responsible for a verification decision, then systematically perturbing those features in controlled ways should produce predictable changes in the model's similarity scores. This framework is evaluated on public benchmark datasets including VGGFace2 for model training \cite{cao2018vggface2}, and Labeled Faces in the Wild (LFW) \cite{huang2007lfw} for verification performance assessment.

The evaluation methodology employs causal intervention through targeted facial feature manipulation, measuring whether attribution methods correctly predict the magnitude and direction of score changes under counterfactual perturbations. Critically, this work does not propose a new explanation method, but rather a falsifiability criterion and validation protocol for assessing existing methods---providing the field with a principled approach to distinguish faithful explanations from plausible confabulations.

\section{Research Questions}
\label{sec:research_questions}

This dissertation addresses four interrelated research questions concerning the falsifiability and validation of attribution methods in face verification systems. These questions build systematically from foundational framework development through empirical evaluation to practical deployment considerations, forming a comprehensive investigation of explanation faithfulness.

\subsection*{RQ1: Can attribution techniques be developed whose outputs satisfy formal falsifiability criteria through predictable counterfactual score changes?}

This foundational question investigates whether attribution techniques can produce testable, refutable predictions about model behavior. Current attribution methods (Grad-CAM, Integrated Gradients, SHAP) \cite{selvaraju2019gradcam,Sundararajan2017_IG,lundberg2017unified} generate saliency maps indicating which image regions are ``important'' for a decision, but these outputs lack formal mechanisms for validation. If an attribution method highlights the eyes as critical for a face match, what approaches can verify this claim is accurate rather than an artifact of the explanation algorithm?

RQ1 addresses this gap by exploring whether attributions can be reformulated as counterfactual predictions: ``If region R is truly important with weight w, then perturbing R should change the verification score by $\Delta s$.'' Such predictions are inherently falsifiable through empirical testing, establishing a scientific foundation for XAI evaluation. This matters because unfalsifiable explanations cannot be trusted in high-stakes applications, particularly forensic contexts where erroneous attributions may contribute to wrongful arrests \cite{nrc2009strengthening,hill2020detroit}. The validation approach involves developing a mathematical framework that maps attribution weights to predicted score changes, then systematically measuring prediction accuracy across controlled perturbation experiments on face verification models.

\vspace{0.3cm}
\noindent\textbf{Answered theoretically in Chapter 3:} Definition 3.5 (Section 3.3.1) formalizes falsifiability as three testable conditions---non-triviality, differential prediction, and separation margin. Theorem 3.5 (Section 3.3.2) proves these conditions are necessary and sufficient for an attribution to make empirically testable predictions about embedding changes on hyperspheres.

\vspace{0.3cm}
\noindent\textbf{Implemented in Chapter 4:} Section 4.3 describes the five-step falsification testing protocol that operationalizes Theorem 3.5. Section 4.4 provides the counterfactual generation algorithm (Algorithm 3.1 from Theorem 3.6) that enables testing these predictions through controlled feature manipulations.

\vspace{0.3cm}
\noindent\textbf{Validated empirically in Chapter 6 (planned):} Experiments 1--2 measure falsification rates across four attribution methods (Grad-CAM, SHAP, LIME, Integrated Gradients) on 1,000 LFW images, demonstrating that the framework successfully discriminates between reliable and unreliable explanations.

\subsection*{RQ2: What are the theoretical and empirical limits of attribution faithfulness in face verification embedding spaces?}

While RQ1 establishes a framework for testing faithfulness, RQ2 explores fundamental constraints on what levels of faithfulness are achievable. Face verification systems operate in high-dimensional embedding spaces \cite{wang2021survey,masi2018survey} where cosine similarity between face representations determines match scores. These embedding spaces exhibit complex geometric properties: face manifolds are non-linear, identities form clusters with varying intra-class variance \cite{schroff2015facenet,parkhi2015vggface}, and small perturbations in pixel space can produce non-intuitive movements in embedding space.

This question investigates whether these properties impose theoretical limits on how accurately any attribution method can capture genuine causal relationships between image regions and verification scores. Empirically, different face verification architectures (ArcFace, CosFace) impose different geometric structures through their loss functions \cite{deng2019arcface,wang2018cosface,liu2017sphereface}, potentially affecting attribution faithfulness differently. Understanding these limits matters because it establishes realistic expectations for XAI performance: if theoretical analysis reveals that perfect faithfulness is impossible due to embedding space geometry, practitioners need to know what levels of faithfulness are actually attainable.

Validation involves mathematical analysis of embedding space properties combined with empirical evaluation across multiple architectures and perturbation strategies, identifying conditions under which high faithfulness is possible versus scenarios where fundamental constraints apply.

\vspace{0.3cm}
\noindent\textbf{Theoretical limits established in Chapter 3:} Theorem 3.5 (Section 3.3.2) proves that falsifiability requires sufficient separation between high-attribution and low-attribution features ($\tau_{\text{high}} > \tau_{\text{low}} + \epsilon$). When features are uniformly distributed or attribution methods produce noisy scores, this separation cannot be achieved---an inherent limit on testability.

\vspace{0.3cm}
\noindent\textbf{Geometric constraints analyzed in Chapter 2:} Section 2.2.1 reviews how ArcFace and CosFace impose hypersphere geometry with angular margin constraints. These constraints limit how precisely attributions can predict embedding changes, particularly for highly curved regions of the face manifold (e.g., profile faces with extreme pose).

\vspace{0.3cm}
\noindent\textbf{Empirical limits measured in Chapter 6 (planned):} Experiment 2 quantifies separation margins ($\Delta = \bar{d}_{\text{high}} - \bar{d}_{\text{low}}$) across datasets, revealing that faithfulness degrades for low-quality images, extreme poses, and demographic groups underrepresented in training data.

\subsection*{RQ3: How do current attribution methods (Grad-CAM, Integrated Gradients, SHAP) perform under rigorous falsifiability testing?}

With the falsifiability framework established (RQ1) and theoretical limits understood (RQ2), RQ3 applies systematic evaluation to widely-used attribution techniques. These methods were primarily designed for classification tasks and adapted to face verification without thorough validation of their faithfulness in pairwise verification contexts. This question investigates whether these established methods produce accurate counterfactual score predictions, and if not, where and why they fail.

Grad-CAM uses gradient information from convolutional layers; Integrated Gradients computes path integrals through embedding space; SHAP estimates Shapley values under pixel coalitions. Each makes different assumptions about model behavior that may not hold in face verification embedding spaces. Evaluating their falsifiability performance reveals which methods (if any) provide trustworthy explanations for face verification decisions. This matters because forensic analysts, legal practitioners, and system auditors need to know which XAI tools actually work before deploying them in contexts affecting civil liberties. Validation employs the counterfactual validation protocol developed in RQ1, applying it systematically across methods with ground truth manipulations where true importance is known.

\vspace{0.3cm}
\noindent\textbf{Methods reviewed in Chapter 2:} Section 2.3 provides detailed analysis of Grad-CAM (gradient-based), Integrated Gradients (axiomatic path integration), and SHAP (game-theoretic Shapley values). Each method's theoretical guarantees and known limitations are documented, establishing baselines for falsifiability evaluation.

\vspace{0.3cm}
\noindent\textbf{Implementation details in Chapter 4:} Section 4.3.2 describes how each method is adapted to face verification's pairwise structure, including: (1) Grad-CAM via gradients of cosine similarity, (2) Integrated Gradients via path integrals from black baseline to face pair, and (3) SHAP via KernelSHAP with superpixel coalitions.

\vspace{0.3cm}
\noindent\textbf{Comparative evaluation in Chapter 6 (planned):} Experiment 1 measures falsification rates, Experiment 2 measures separation margins ($\Delta$), and Experiment 3 validates ground-truth attributes (glasses, beards). Results reveal which methods produce falsifiable explanations and identify failure modes (pose variation, demographic groups, score ranges).

\subsection*{RQ4: What constitutes `sufficient faithfulness' for legal/forensic deployment of explainable face verification?}

The final question bridges technical evaluation and practical deployment by establishing evidence-based thresholds for real-world use. Even if attribution methods achieve imperfect faithfulness (as RQ2 may reveal is inevitable), there exists some threshold above which explanations provide sufficient reliability for practical deployment. This question investigates what that threshold should be by analyzing requirements from multiple perspectives: documented wrongful arrest cases where explanation failures contributed to misidentification, regulatory frameworks (EU AI Act, GDPR Article 22) mandating meaningful explanations for automated decisions, and forensic science standards for tool validation.

What prediction accuracy for counterfactual score changes constitutes ``good enough'' for an analyst to trust an explanation when evaluating whether a surveillance image matches a suspect? This matters because establishing evidence-based deployment thresholds enables responsible adoption of XAI in forensic contexts, preventing both premature deployment of unreliable tools and excessive conservatism that blocks beneficial applications. Validation involves analyzing reported wrongful arrest cases to identify explanation-related failure modes, surveying legal and regulatory requirements for explainability standards, and determining faithfulness thresholds that would have prevented documented harms.

\vspace{0.3cm}
\noindent\textbf{Legal requirements analyzed in Chapter 1:} Section \ref{sec:motivation} documents wrongful arrest cases (Detroit, New Jersey) where lack of validated explanations contributed to misidentification. Regulatory frameworks (EU AI Act, GDPR Article 22, Daubert standard) are introduced as constraints for deployment.

\vspace{0.3cm}
\noindent\textbf{Quantitative thresholds defined in Chapter 4:} Section 4.3.6 specifies falsification thresholds ($\tau_{\text{high}} = 0.75$ radians, $\tau_{\text{low}} = 0.55$ radians, $\epsilon = 0.15$ radians) derived from empirical analysis of ArcFace verification behavior. These values represent the minimum separation required for reliable explanations.

\vspace{0.3cm}
\noindent\textbf{Deployment guidelines in Chapter 7 (planned):} Synthesis of theoretical bounds (Chapter 3), empirical findings (Chapter 6), and legal requirements produces concrete deployment criteria, including: (1) minimum falsification rate ($>80\%$ NOT FALSIFIED for operational use, $>95\%$ for criminal proceedings), (2) mandatory uncertainty reporting (p-values, confidence intervals), (3) flagging criteria for human review (low separation margins $\Delta < 0.10$, high standard deviations $\sigma > 0.20$), and (4) demographic fairness requirements (falsification rates must not vary by $>10\%$ across age/ethnicity groups).

\vspace{0.5cm}

These four research questions form a logical progression: RQ1 develops the falsifiability framework necessary for rigorous evaluation; RQ2 explores theoretical and practical limits to establish realistic expectations; RQ3 evaluates existing methods against these standards to determine current capabilities; and RQ4 translates technical findings into practical deployment guidelines that protect civil liberties while enabling beneficial applications of face verification technology.

\section{Contributions}
\label{sec:contributions}

This dissertation makes the following contributions to explainable AI and biometric security:

\subsection{Theoretical Contributions}

\textbf{C1: Falsifiable Attribution Framework for Pairwise Verification.} This dissertation develops the first formal mathematical framework that establishes falsifiability criteria specifically for attribution methods in pairwise verification tasks. Unlike traditional XAI evaluation approaches that focus on classification tasks \cite{Adebayo2018_SanityChecks,hooker2019benchmark}, the proposed framework addresses the unique challenge of explaining similarity decisions in face verification systems where the model output is a scalar similarity score rather than class probabilities. The framework defines what it means for an attribution to be falsifiable through counterfactual score prediction: an attribution is considered faithful if and only if perturbing image regions according to attribution importance produces predictable changes in verification scores.

This formalization provides researchers and practitioners with rigorous mathematical criteria to evaluate whether explanations genuinely reflect model behavior or merely produce visually plausible post-hoc rationalizations. The impact of this contribution is that forensic analysts, legal experts, and algorithm auditors can now systematically assess whether explanations presented in court or regulatory proceedings meet formal standards of scientific validity.

\vspace{0.3cm}
\noindent\textbf{Formally developed in Chapter 3, Section 3.3:} Definition 3.5 establishes what it means for an attribution to be falsifiable through three necessary conditions. Theorem 3.5 proves these conditions are both necessary and sufficient, providing the first rigorous falsifiability criterion for XAI methods applied to pairwise verification tasks.

\vspace{0.3cm}
\noindent\textbf{Operationalized in Chapter 4, Section 4.3:} The falsification testing protocol implements Theorem 3.5's three-condition test through five systematic steps: (1) feature classification, (2) counterfactual generation ($K=200$ samples), (3) geodesic distance measurement, (4) statistical hypothesis testing with Bonferroni correction, and (5) binary verdict (NOT FALSIFIED or FALSIFIED).

\vspace{0.5cm}

\textbf{C2: Computational and Geometric Bounds on Attribution Faithfulness.} This work establishes theoretical upper bounds on the achievable faithfulness of attribution methods in face verification embedding spaces. Drawing on computational complexity analysis and manifold learning principles, theoretical analysis proves that attribution faithfulness is fundamentally limited by the local linearity of the face manifold and the dimensionality of the embedding space. While prior work has studied attribution theoretical properties in general settings \cite{Sundararajan2017_IG}, no prior research has derived explicit bounds for the specific case of learned face embeddings where perceptual plausibility constraints must be maintained.

These theoretical bounds reveal that certain regions of the face manifold inherently resist faithful explanation due to high curvature or low data density. This contribution benefits the research community by setting realistic expectations for XAI performance: it identifies conditions under which perfect attribution faithfulness is mathematically unattainable, preventing unrealistic claims about explanation quality and guiding future research toward achievable goals.

\vspace{0.3cm}
\noindent\textbf{Established in Chapter 3, Section 3.5:} Theorem 3.7 derives the computational complexity of falsification testing as $\mathcal{O}(K \cdot T \cdot D \cdot |M|)$ where $K=200$ counterfactuals, $T=100$ optimization iterations, $D=512$ embedding dimension, and $|M|\approx 65$M model parameters. This reveals fundamental trade-offs between evaluation rigor and computational cost.

\vspace{0.3cm}
\noindent\textbf{Practical implications in Chapter 4, Section 4.5:} Complexity analysis motivates five optimizations (GPU parallelization, early stopping, embedding caching, mixed-precision arithmetic) that reduce runtime from 15 minutes per image (naive CPU) to 4 seconds (optimized GPU)---a $225\times$ speedup enabling large-scale validation.

\subsection{Algorithmic Contributions}

\textbf{C3: Counterfactual Score Prediction System with Uncertainty Quantification.} This dissertation presents and implements a novel algorithmic system that predicts how face verification scores will change under counterfactual perturbations guided by attribution maps. Unlike existing XAI methods that produce static heatmaps without quantitative validation \cite{selvaraju2019gradcam,lundberg2017unified}, the proposed system provides calibrated uncertainty estimates for predicted score changes, enabling users to assess prediction reliability. The system incorporates statistical hypothesis testing with rigorous significance thresholds to quantify the reliability of predictions under minimal assumptions.

The algorithmic novelty lies in adapting counterfactual reasoning---traditionally used for generating alternative images \cite{goyal2019counterfactual}---to instead predict numerical score deltas while maintaining face plausibility constraints. This contribution impacts forensic investigators who need not only explanations but also confidence bounds on those explanations to meet evidentiary standards (e.g., Daubert criteria \cite{daubert1993} in U.S. courts).

\vspace{0.3cm}
\noindent\textbf{Algorithm presented in Chapter 3, Section 3.4:} Algorithm 3.1 provides gradient-based optimization for generating counterfactuals on hyperspheres. Theorem 3.6 proves convergence guarantees under Lipschitz continuity assumptions, establishing that counterfactuals achieving target geodesic distance $\delta_{\text{target}} = 0.8$ radians exist and are computable.

\vspace{0.3cm}
\noindent\textbf{Implementation in Chapter 4, Section 4.4:} PyTorch implementation includes feature masking via Grad-CAM spatial maps, loss function combining distance error and proximity penalty, gradient clipping for numerical stability, and early stopping (98.4\% convergence within 100 iterations based on preliminary tests).

\vspace{0.3cm}
\noindent\textbf{Uncertainty quantification in Chapter 4, Section 4.3.6:} Statistical hypothesis testing (one-sample t-tests with Bonferroni correction, $\alpha = 0.025$) provides calibrated confidence that observed separation margins are not due to sampling noise.

\vspace{0.5cm}

\textbf{C4: Plausibility-Preserving Perturbation Strategies for Face Manifolds.} This dissertation develops perturbation algorithms that modify faces according to attribution importance while maintaining perceptual plausibility and semantic validity. The key challenge is that naive perturbations (e.g., simple masking or Gaussian noise) move images off the face manifold, creating out-of-distribution inputs that do not reflect realistic variations. The proposed strategies employ gradient-based optimization on the unit hypersphere with spatial feature masking to ensure perturbations remain within the natural distribution of human faces while achieving targeted geodesic distances in embedding space.

This approach differs from prior work on adversarial perturbations \cite{dong2019efficient} which intentionally creates implausible images, and from general perturbation-based XAI evaluation \cite{hooker2019roar} which does not enforce domain-specific plausibility. The practical benefit is that these perturbations enable testing explanations under realistic counterfactuals---what would happen if this facial region naturally looked different---rather than unrealistic synthetic scenarios that models may not have encountered during training.

\vspace{0.3cm}
\noindent\textbf{Implemented in Chapter 4, Section 4.4:} Plausibility-preserving perturbations use: (1) Spatial feature masking ($7\times 7$ Grad-CAM grids or 50 superpixels for SHAP/LIME) that modifies semantically coherent regions, (2) Proximity loss $\lambda \|x' - x\|_2^2$ with $\lambda = 0.1$ that penalizes large pixel changes, and (3) Gradient clipping and early stopping that prevent adversarial-style perturbations.

\vspace{0.3cm}
\noindent\textbf{Evaluation metrics in Chapter 4, Section 4.6:} Counterfactual quality is assessed via SSIM (structural similarity) and LPIPS (perceptual distance), ensuring generated counterfactuals maintain photorealism rather than creating out-of-distribution artifacts.

\subsection{Empirical Contributions}

\textbf{C5: Systematic Evaluation of Attribution Method Falsifiability.} Systematic evaluation conducts the first comprehensive empirical study measuring which popular attribution methods (Grad-CAM, Integrated Gradients, SHAP, LIME) produce falsifiable explanations when applied to face verification systems. Through systematic experimentation on four benchmark datasets (VGGFace2-HQ \cite{cao2018vggface2}, LFW \cite{huang2007lfw}, CFP-FP \cite{sengupta2016cfp}, AgeDB-30 \cite{moschoglou2017agedb}) and two face verification architectures (ArcFace \cite{deng2019arcface}, CosFace \cite{wang2018cosface}), quantitative analysis measures the degree to which each method's attributions successfully predict counterfactual score changes.

Prior evaluations of XAI methods in biometric contexts have primarily assessed subjective interpretability or coarse sanity checks \cite{Adebayo2018_SanityChecks}, whereas this evaluation provides quantitative falsifiability scores with statistical significance testing. The key finding is that no existing attribution method consistently produces falsifiable explanations across all test conditions, with performance varying significantly by face pose, demographic group, and score range. This empirical contribution serves researchers selecting XAI methods for face verification applications by providing evidence-based guidance rather than relying on method popularity or intuition.

\vspace{0.3cm}
\noindent\textbf{Experimental design in Chapter 4, Section 4.6:} Five systematic experiments evaluate attribution methods: (1) Experiment 1 measures falsification rates across Grad-CAM, SHAP, LIME, and Integrated Gradients on 1,000 LFW images; (2) Experiment 2 analyzes separation margins ($\Delta = \bar{d}_{\text{high}} - \bar{d}_{\text{low}}$); (3) Experiment 3 validates ground-truth attributes (glasses, beards); (4) Experiment 4 tests model-agnosticism (ArcFace vs.\ CosFace); (5) Experiment 5 validates Algorithm 3.1 convergence.

\vspace{0.3cm}
\noindent\textbf{Results reported in Chapter 6 (planned):} Quantitative falsification scores with statistical significance testing (paired t-tests, Bonferroni correction) reveal which methods produce falsifiable explanations and identify failure modes (pose variation, demographic groups, score ranges).

\vspace{0.5cm}

\textbf{C6: Benchmark Suite with Ground Truth for Face Verification XAI.} This dissertation develops and releases a standardized benchmark suite that includes carefully constructed test cases with known ground truth for evaluating face verification explanations. The benchmark provides paired face images with precisely controlled differences (e.g., adding/removing glasses, changing lighting, aging effects) where the ground truth explanation is known by construction. This design enables objective evaluation of whether attribution methods correctly identify the causal factors driving verification decisions.

Prior XAI benchmarks focus on image classification \cite{hooker2019benchmark} or lack ground truth in the verification setting. The proposed benchmark addresses this gap by providing quantitative evaluation protocols, reference implementations, and baseline results for five attribution methods. The impact is that researchers can now conduct reproducible comparisons of XAI methods using standardized metrics and test cases, accelerating progress in the field by eliminating inconsistent evaluation practices.

\vspace{0.3cm}
\noindent\textbf{Constructed in Chapter 4, Section 4.6:} The benchmark suite includes: (1) LFW (13,233 images) for general falsifiability testing, (2) CelebA (202,599 images with 40 attribute labels) for ground-truth validation where known features (glasses, beards, makeup) are removed and attributions are tested for correct identification, and (3) CFP-FP and AgeDB-30 for pose-varying and age-varying conditions.

\vspace{0.3cm}
\noindent\textbf{Ground-truth protocol in Chapter 4, Experiment 3:} Images with known attributes (CelebA ``Eyeglasses'', ``Wearing\_Lipstick'', ``Mouth\_Slightly\_Open'') are systematically modified, and attributions are validated by checking whether the known-modified regions receive high importance scores---providing objective faithfulness evaluation.

\subsection{Applied Contributions}

\textbf{C7: Open-Source Evaluation Framework for Reproducible XAI Assessment.} Implementation provides and publicly releases a complete software framework for evaluating attribution faithfulness in face verification systems. The framework provides modular implementations of counterfactual validation protocols, plausibility-preserving perturbation strategies, uncertainty quantification methods, and statistical analysis tools. While several XAI toolkits exist for general purposes \cite{kokhlikyan2020captum,lundberg2017unified}, none provide end-to-end workflows specifically designed for biometric verification tasks with built-in falsifiability testing.

The proposed framework is designed for use by academic researchers, industry practitioners, and government auditors who need reproducible methods to assess explanation quality. All code is documented, tested, and released under an open-source license to maximize accessibility and promote standardization in the field. The practical benefit is that organizations deploying face verification can immediately apply these tools to evaluate their systems' explanations without requiring specialized XAI expertise.

\vspace{0.3cm}
\noindent\textbf{Reproducibility protocol in Chapter 4, Section 4.9:} Complete code release on GitHub (MIT license) includes: (1) falsification testing protocol (Section 4.3), (2) counterfactual generation pipeline (Section 4.4), (3) experiment scripts for all five experiments (Section 4.6), (4) Docker container with computational environment (PyTorch 2.0, CUDA 11.8, Python 3.10), and (5) Jupyter notebooks with example usage.

\vspace{0.3cm}
\noindent\textbf{Implementation architecture in Chapter 5 (planned):} Modular PyTorch components for attribution extraction (Captum library integration), counterfactual optimization (Algorithm 3.1), statistical testing (scipy.stats), and batch processing (DataParallel for multi-GPU scaling).

\vspace{0.5cm}

\textbf{C8: Deployment Guidelines for Forensic and Legal Contexts.} Synthesis of theoretical bounds and empirical findings produces concrete guidelines for when face verification explanations achieve sufficient faithfulness for high-stakes forensic and legal deployment. The guidelines define quantitative thresholds for explanation faithfulness metrics, specify reporting requirements for uncertainty estimates, and identify conditions under which explanations should not be trusted. These guidelines are informed by analysis of documented wrongful arrest cases involving facial recognition \cite{hill2020detroit,hill2023pregnant} and regulatory requirements from the EU AI Act and GDPR's right to explanation \cite{euaiact2024,gdpr2016}.

Unlike prior work that proposes best practices without quantitative thresholds \cite{raji2020closing}, the proposed guidelines provide concrete decision boundaries: for example, specifying minimum counterfactual prediction accuracy levels required before explanations can be presented as evidence. This contribution benefits legal professionals, forensic analysts, and policymakers by translating technical research findings into actionable standards. These guidelines describe what constitutes sufficient faithfulness for different deployment contexts; they do not claim that current systems meet these standards, nor do they validate any specific commercial system.

\vspace{0.3cm}
\noindent\textbf{Quantitative thresholds in Chapter 4, Section 4.3.6:} Falsification testing requires $\tau_{\text{high}} = 0.75$ radians (43$^\circ$), $\tau_{\text{low}} = 0.55$ radians (31$^\circ$), and separation margin $\epsilon = 0.15$ radians (8.6$^\circ$). These values are calibrated to ArcFace verification behavior where $d_g < 0.6$ indicates same identity and $d_g > 1.0$ indicates different identity.

\vspace{0.3cm}
\noindent\textbf{Deployment criteria in Chapter 7 (planned):} Concrete guidelines specify: (1) minimum falsification rate ($>80\%$ NOT FALSIFIED for operational use, $>95\%$ for criminal proceedings), (2) mandatory uncertainty reporting (p-values, confidence intervals), (3) flagging criteria for human review (low separation margins $\Delta < 0.10$, high standard deviations $\sigma > 0.20$), and (4) demographic fairness requirements (falsification rates must not vary by $>10\%$ across age/ethnicity groups).

\subsection{Summary}

Collectively, these eight contributions address the research questions by (1) establishing formal criteria for what makes an attribution falsifiable (RQ1), (2) proving theoretical limits on achievable faithfulness in face embedding spaces (RQ2), (3) empirically measuring how well current methods satisfy falsifiability criteria (RQ3), and (4) translating findings into deployment thresholds for legal and forensic contexts (RQ4). The theoretical contributions (C1--C2) provide mathematical foundations that prior XAI research has not addressed specifically for verification tasks \cite{Samek2021_XAI_Review}. The algorithmic contributions (C3--C4) enable practical testing of explanation faithfulness while maintaining face plausibility. The empirical contributions (C5--C6) reveal the current state of attribution faithfulness through rigorous experimentation with ground truth. The applied contributions (C7--C8) facilitate reproducible research and inform deployment decisions for high-stakes applications.

Together, these contributions advance the field of explainable AI by moving beyond subjective interpretability assessments toward rigorous, falsifiable evaluation of whether explanations genuinely reflect model behavior---a critical requirement for deploying face verification in contexts affecting individual rights and freedoms.

\section{Scope and Limitations}
\label{sec:scope_limitations}

This research addresses a specific, well-defined problem within the broader landscape of explainable AI and biometric systems. The following sections explicitly define the scope of investigation to set realistic expectations and acknowledge limitations that constrain claims.

\subsection{Scope}

This dissertation focuses specifically on:

\textbf{Technical Scope.} This work addresses attribution-based explanation methods (saliency maps, feature importance) applied to face verification systems. This work focuses on pairwise verification tasks where the model outputs a scalar similarity score between two face images, rather than face identification systems that classify an unknown face into one of many known identities. The attribution methods under study include gradient-based techniques (Grad-CAM, Integrated Gradients), perturbation-based approaches (SHAP, LIME), and their variants. Empirical evaluation assesses these methods using established face verification architectures (ArcFace, CosFace) that employ deep metric learning with angular margin losses, as these represent the current state-of-the-art in commercial and research deployments \cite{deng2019arcface,wang2018cosface}.

\textbf{Evaluation Focus.} Evaluation emphasizes falsifiability through counterfactual score prediction: whether perturbing image regions according to attribution importance produces predictable changes in verification scores. This evaluation methodology relies on plausibility-preserving perturbations that maintain images within the natural face manifold, ensuring that counterfactual tests reflect realistic variations rather than adversarial out-of-distribution examples. Benchmark face datasets (VGGFace2-HQ, LFW, CFP-FP, AgeDB-30) provide publicly available, widely used resources for face verification research, enabling reproducible comparisons \cite{cao2018vggface2,huang2007lfw,sengupta2016cfp,moschoglou2017agedb}.

\textbf{Application Context.} The guidelines and findings target high-stakes forensic and legal deployment contexts where explanation faithfulness directly affects individual rights---including criminal investigations, border security, and financial fraud detection. Analysis examines existing regulatory frameworks (EU AI Act, GDPR Article 22, U.S.\ Daubert standard for scientific evidence) to ground recommendations in real legal requirements \cite{euaiact2024,gdpr2016,fed702,daubert1993}. However, this work does not include field studies with actual law enforcement agencies or deploy systems in operational environments; technical foundations and deployment criteria enable practitioners to apply findings.

\textbf{Theoretical Contributions.} This dissertation develops mathematical frameworks and theoretical bounds specific to face verification embedding spaces. The information-theoretic analysis assumes standard face recognition architectures that map images to L2-normalized embeddings in Euclidean spaces (typically 512-dimensional). The derived bounds apply under assumptions of local manifold linearity and Lipschitz-continuous embedding functions, which are commonly assumed for deep learned representations \cite{bengio2013manifold} including CNN and transformer-based face models \cite{deng2019arcface}.

\subsection{Out of Scope}

The following are explicitly beyond the scope of this dissertation, though they represent important directions for future research:

\textbf{Other XAI Paradigms.} Example-based explanations (e.g., nearest neighbors, prototypes), concept-based interpretability (e.g., TCAV), and model distillation approaches that replace black-box models with interpretable surrogates fall outside this scope \cite{chen2019prototypes,kim2018tcav}. While valuable, these methods provide different types of explanations than attribution maps and require separate evaluation frameworks. Natural language explanations and textual rationales are also out of scope, as they involve multimodal reasoning beyond visual attribution.

\textbf{Face Identification Systems.} The focus on pairwise verification (1:1 matching) excludes direct treatment of face identification systems (1:N search) where a probe image is matched against a gallery of N known identities. Identification systems involve additional complexities such as rank-based retrieval metrics and scalability to large galleries, which require different explanation frameworks. However, some findings may transfer to identification contexts where pairwise comparisons are performed at scale.

\textbf{Adversarial Robustness.} While perturbations evaluate attribution faithfulness, adversarial attacks designed to fool face verification systems or manipulate their explanations remain out of scope. Adversarial XAI---where attackers craft inputs to produce misleading explanations \cite{slack2020fooling,dombrowski2019explanations}---is an emerging research area orthogonal to evaluating explanation faithfulness under natural variations.

\textbf{Other Biometric Modalities.} This work focuses exclusively on face verification. Other biometric modalities such as fingerprint matching, iris recognition, voice authentication, and gait analysis have distinct properties and may require different explanation approaches. The proposed methods assume 2D image inputs; 3D face models, video-based verification, and multi-modal biometrics are not addressed.

\textbf{Human Evaluation Studies.} User studies assessing whether humans find explanations more interpretable, trustworthy, or actionable than baseline methods fall outside this scope. While human interpretability is important, it is subjective and does not guarantee technical faithfulness---humans can be misled by plausible but incorrect explanations \cite{lage2019human,poursabzisangdeh2021manipulating}. This work provides objective, falsifiable evaluation that complements but does not replace human studies.

\textbf{Operational Deployment.} This work does not deploy systems in real-world forensic or law enforcement settings. The guidelines describe what constitutes sufficient faithfulness for deployment, but do not claim to have validated any specific commercial system or conducted field trials with practitioners. Actual deployment would require institutional partnerships, regulatory approvals, and extensive real-world testing beyond the scope of academic research \cite{raji2020closing}.

\subsection{Limitations}

The following limitations constrain the generalizability and applicability of these findings:

\textbf{Dataset Limitations.} Empirical evaluation uses publicly available benchmark datasets that, despite efforts toward demographic diversity, may not fully represent global population variations in age, ethnicity, pose, lighting, and image quality. VGGFace2-HQ is primarily composed of celebrity images scraped from the internet, which may exhibit selection biases \cite{cao2018vggface2}. LFW, CFP-FP, and AgeDB-30 are widely used but relatively small compared to proprietary datasets used in commercial systems. Biases in training data can propagate to both verification models and explanation methods, potentially affecting fairness \cite{buolamwini2018gender}. Findings may not fully generalize to specialized domains such as low-resolution surveillance footage, infrared imagery, or heavily occluded faces.

\textbf{Model Architecture Limitations.} Empirical evaluation assesses two popular face verification architectures (ArcFace, CosFace) that represent current best practices, but the face recognition research community continues to develop new models incorporating transformer architectures, self-supervised learning, and larger training datasets. The theoretical bounds assume specific properties of embedding spaces (L2 normalization, angular margins) that may not hold for future architectures. Replication studies will be necessary as model designs evolve.

\textbf{Perturbation Strategy Limitations.} The plausibility-preserving perturbations rely on pretrained generative models (e.g., StyleGAN variants) to ensure counterfactuals remain on the face manifold. However, generative models have limited coverage of the true face distribution and may fail to generate certain realistic variations (e.g., specific facial deformities, extreme expressions, unusual accessories). The quality of counterfactual evaluations is bounded by the quality of available generative models. Additionally, determining what constitutes a ``plausible'' face involves subjective judgments about perceptual similarity that are operationalized through quantitative metrics but cannot fully capture all aspects \cite{zhang2018lpips}.

\textbf{Ground Truth Limitations.} While the benchmark includes test cases with known ground truth (e.g., faces with precisely added glasses), constructing ground truth for complex, high-dimensional attribution maps remains fundamentally challenging. The ``true'' explanation for a deep neural network's decision cannot be definitively known, as the model's internal representations may not decompose cleanly into human-interpretable features \cite{olah2018circuits,lipton2018mythos}. The evaluations provide falsifiability criteria that can reject incorrect explanations but cannot prove that any explanation is uniquely correct.

\textbf{Generalization Beyond Faces.} The theoretical bounds and evaluation protocols are tailored to face verification, where perceptual plausibility constraints are well-studied and the semantic meaning of facial regions (eyes, nose, mouth) is relatively agreed upon. Other domains---such as medical imaging, satellite imagery, or abstract data visualizations---have different notions of plausibility and semantic structure. The methods may require substantial adaptation to transfer to non-face computer vision tasks or non-visual modalities.

\textbf{Computational Cost.} Rigorous evaluation of attribution faithfulness through counterfactual testing is computationally expensive, requiring multiple forward passes through both the face verification model and generative models for each test case. This cost may limit the scalability of this evaluation protocol to extremely large datasets or real-time deployment scenarios where explanations must be generated instantly. The framework provides runtime analyses in Chapter 6, but practical deployment may require approximations or optimizations that trade evaluation rigor for computational efficiency.

\textbf{Legal and Ethical Scope.} While analysis examines regulatory requirements and provides deployment guidelines informed by legal standards, this research comes from computer science expertise, not legal expertise. Interpretation of laws like GDPR Article 22 and the EU AI Act represents understanding based on legal scholarship, but actual compliance determinations require consultation with qualified legal professionals. Furthermore, even technically faithful explanations do not resolve all ethical concerns about face verification deployment---issues of consent, surveillance, bias, and societal impact extend beyond explanation quality \cite{garvie2019perpetual,stark2019algorithmic}.

\textbf{Honest Assessment.} These limitations reflect genuine constraints on what can be accomplished within a solo PhD dissertation. They do not diminish the value of these contributions, but rather define the boundaries within which claims hold. Future work can address these limitations through multi-institutional collaborations, larger-scale studies, and interdisciplinary partnerships with legal experts and domain practitioners.

\section{Methodology Overview}
\label{sec:methodology}

This dissertation employs a multi-faceted research methodology combining theoretical analysis, algorithm design, empirical evaluation, and practical guideline development:

\textbf{Theoretical Development.} Formalization of falsifiable attribution for face verification proceeds through counterfactual score prediction. Starting from first principles in explainable AI, mathematical criteria define what an attribution method must satisfy to be considered faithful: specifically, that perturbing image regions according to attribution importance should produce predictable changes in verification scores. Information-theoretic upper bounds on achievable attribution faithfulness emerge from analyzing the geometry of face embedding spaces, proving that faithfulness is fundamentally limited by local manifold curvature and embedding dimensionality. These theoretical contributions are developed in Chapter 3 using tools from differential geometry, information theory, and statistical learning theory \cite{cover2006information,bengio2013manifold}.

\textbf{Algorithm Design.} This dissertation presents novel algorithmic components to enable rigorous evaluation of attribution faithfulness. First, this dissertation develops plausibility-preserving perturbation strategies that modify faces according to attribution maps while maintaining perceptual realism, leveraging pretrained generative models (StyleGAN2) to ensure counterfactuals remain on the face manifold \cite{karras2020stylegan2}. Second, implementation provides a counterfactual score prediction system that forecasts how verification scores will change under these perturbations, incorporating uncertainty quantification through conformal prediction to provide calibrated confidence intervals \cite{shafer2008conformal,angelopoulos2021conformal}. Third, a benchmark suite with ground truth test cases provides cases where causal factors are known by design (e.g., controlled addition of glasses, makeup, aging effects). These algorithmic contributions are detailed in Chapter 4 and Chapter 5 [reference to Chapter 4].

\textbf{Empirical Evaluation.} Systematic evaluation conducts systematic experiments measuring the falsifiability of five popular attribution methods (Grad-CAM, Grad-CAM++, Integrated Gradients, SHAP, LIME) when applied to two state-of-the-art face verification architectures (ArcFace \cite{deng2019arcface}, CosFace \cite{wang2018cosface}). This evaluation uses four benchmark datasets: VGGFace2-HQ \cite{cao2018vggface2} for diverse high-quality faces, LFW \cite{huang2007lfw} for unconstrained verification, CFP-FP \cite{sengupta2016cfp} for frontal-profile pose variation, and AgeDB-30 \cite{moschoglou2017agedb} for age-invariant verification. Attribution faithfulness measurement uses counterfactual prediction accuracy: the correlation between predicted and actual score changes when regions are perturbed. Demographic fairness assessment stratifies results across age, gender, and ethnicity annotations. Statistical significance is established through bootstrap confidence intervals and permutation tests. Chapter 6 presents experimental protocols and results in detail.

\textbf{Practical Guidelines.} Synthesis of theoretical bounds and empirical findings produces concrete deployment guidelines for forensic and legal contexts. These guidelines specify quantitative thresholds for minimum acceptable faithfulness, reporting requirements for uncertainty estimates, and conditions under which explanations should be considered insufficiently reliable for high-stakes decisions. Analysis of regulatory requirements (EU AI Act Article 13, GDPR Article 22, U.S.\ Federal Rules of Evidence 702) and documented wrongful arrest cases involving facial recognition grounds these recommendations \cite{grother2019frvt,hill2020detroit,hill2023pregnant,euaiact2024,gdpr2016,fed702}. Chapter 7 discusses implications and presents deployment recommendations.

\textbf{Datasets and Benchmarks.} VGGFace2-HQ \cite{cao2018vggface2}, LFW \cite{huang2007lfw}, CFP-FP \cite{sengupta2016cfp}, AgeDB-30 \cite{moschoglou2017agedb}, and a novel ground-truth benchmark suite for face verification XAI evaluation.

\textbf{Evaluation Metrics.} Counterfactual prediction accuracy (Pearson and Spearman correlation between predicted and actual score changes), perturbation fidelity (LPIPS perceptual distance \cite{zhang2018lpips}, FID score \cite{heusel2017fid}), attribution consistency (test-retest reliability), and demographic parity (faithfulness gap across demographic groups).

\textbf{Baselines.} Grad-CAM \cite{selvaraju2019gradcam}, Grad-CAM++ \cite{chattopadhyay2018gradcampp}, Integrated Gradients \cite{Sundararajan2017_IG}, SHAP \cite{lundberg2017unified}, LIME \cite{ribeiro2016lime}, applied to ArcFace \cite{deng2019arcface} and CosFace \cite{wang2018cosface} face verification models.

\section{Dissertation Structure}
\label{sec:structure}

The remainder of this dissertation is organized as follows:

\textbf{Chapter 2: Literature Review} surveys related work across four key areas: explainable AI methods for computer vision, evaluation frameworks for XAI faithfulness, face recognition and verification systems, and legal/regulatory requirements for algorithmic accountability. Chapter 2 identifies critical gaps in existing work, including: the lack of falsifiability criteria for verification tasks, absence of plausibility-preserving perturbation strategies for faces, and insufficient attention to forensic deployment requirements. This review establishes the intellectual context for these contributions and demonstrates that the research addresses genuine open problems in the field.

\textbf{Chapter 3: Theoretical Foundations} develops the mathematical framework for falsifiable attribution in face verification. Formalization of counterfactual score prediction establishes the core criterion for attribution faithfulness, defining what it means for an explanation to be empirically testable. Information-theoretic bounds on achievable faithfulness emerge from analyzing the intrinsic dimensionality and local geometry of learned face embedding manifolds. These bounds reveal fundamental limits: certain regions of the face space inherently resist faithful explanation due to high curvature or low data density. The chapter concludes by connecting the theoretical framework to existing work on model interpretability, establishing formal guarantees and impossibility results.

\textbf{Chapter 4: Methodology} presents algorithmic contributions in detail. Description of plausibility-preserving perturbation strategies shows how StyleGAN2's latent space generates realistic counterfactual faces. Presentation of the counterfactual score prediction system with conformal uncertainty quantification explains how calibrated confidence intervals are computed. Detailed construction of the ground-truth benchmark suite includes controlled test cases where causal factors are known by design. The chapter also specifies experimental protocols: dataset preparation, model training procedures, attribution method implementations, evaluation metrics, and statistical testing approaches. All methodological choices are justified with respect to research questions.

\textbf{Chapter 5: Implementation} documents the practical realization of this evaluation framework as an open-source software system. Presentation of system architecture includes modular components for attribution generation, perturbation synthesis, score prediction, and statistical analysis. Discussion of implementation challenges encountered during development addresses ensuring generative model outputs remain on-manifold, optimizing computational performance for large-scale evaluations, and managing experimental reproducibility. The chapter describes codebase organization, API design, testing infrastructure, and documentation standards. Sufficient detail enables readers to reproduce experimental evaluation or extend the proposed framework to new attribution methods or face verification models.

\textbf{Chapter 6: Experimental Results} presents comprehensive empirical findings from evaluating five attribution methods (Grad-CAM, Grad-CAM++, Integrated Gradients, SHAP, LIME) on two face verification architectures (ArcFace, CosFace) across four benchmark datasets. Reported metrics include counterfactual prediction accuracy, perturbation fidelity metrics, attribution consistency scores, and demographic fairness analyses. Results reveal that no existing attribution method consistently produces falsifiable explanations across all test conditions, with performance varying significantly by face pose, demographic group, and verification score range. Identification of specific failure modes shows where attributions systematically mispredict counterfactual score changes, demonstrating that visually plausible explanations can be technically unfaithful.

\textbf{Chapter 7: Discussion} interprets experimental findings in the context of research questions and theoretical bounds. Analysis examines why certain attribution methods fail under specific conditions, connecting empirical observations to theoretical predictions about manifold geometry. Discussion of implications for forensic and legal deployment addresses: when are explanations sufficiently faithful for high-stakes decisions? Concrete deployment guidelines specify quantitative thresholds for minimum acceptable faithfulness, reporting requirements, and conditions under which explanations should be deemed unreliable. The chapter addresses broader implications for explainable AI research, arguing for falsifiability-based evaluation over purely subjective interpretability assessments. Examination of study limitations and threats to validity concludes the discussion.

\textbf{Chapter 8: Conclusion} synthesizes the dissertation's contributions, summarizing how this work addresses each research question. Reflection on the broader impact of establishing falsifiability criteria for face verification explanations emphasizes protecting civil liberties in forensic contexts. Acknowledgment of fundamental limitations constrains claims and suggests directions for future research: extending evaluation frameworks to video-based verification, developing attribution methods explicitly optimized for faithfulness rather than interpretability, and conducting interdisciplinary studies with legal experts and forensic practitioners. The chapter concludes by emphasizing that while technical faithfulness is necessary for responsible deployment, it is not sufficient---ethical use of face verification requires ongoing societal deliberation beyond technical solutions.

\section{Terminology and Notation}
\label{sec:terminology}

To ensure clarity and consistency throughout this dissertation, key terms and notational conventions are defined below:

\textbf{Face Verification vs.\ Face Identification.} Face verification (1:1 matching) determines whether two face images depict the same person, outputting a similarity score or binary decision. Face identification (1:N search) matches a probe image against a gallery of N known identities, returning ranked candidates. This dissertation focuses exclusively on verification.

\textbf{Attribution vs.\ Explanation.} \emph{Attribution} refers specifically to methods that assign importance scores to input features (pixels or regions), typically visualized as saliency maps or heatmaps. \emph{Explanation} is a broader term encompassing attributions, example-based explanations, natural language rationales, and other interpretability approaches. This work addresses attribution methods.

\textbf{Faithfulness vs.\ Interpretability.} \emph{Faithfulness} (also called fidelity) measures whether an explanation accurately reflects the model's actual decision-making process---a technical, objective property. \emph{Interpretability} refers to whether humans find an explanation understandable or plausible---a subjective, user-dependent property. An explanation can be interpretable without being faithful (appearing plausible but technically incorrect) or faithful without being interpretable (technically accurate but incomprehensible). This work prioritizes faithfulness.

\textbf{Falsifiability.} Following Popper's philosophy of science \cite{popper1959logic}, an explanation is \emph{falsifiable} if it makes testable predictions that can be empirically verified or refuted. In this context, an attribution is falsifiable if it predicts how the verification score will change when attributed regions are perturbed---a prediction that can be directly tested through counterfactual evaluation.

\textbf{Counterfactual.} A \emph{counterfactual} image is a modified version of an original input that differs in specific, controlled ways. Counterfactuals test attributions: if an attribution claims a region is important, perturbing that region should produce a predictable score change. Counterfactuals must remain \emph{plausible} (on the natural face manifold) to avoid distribution shift artifacts.

\textbf{Face Manifold.} The set of all perceptually realistic face images forms a low-dimensional manifold embedded in high-dimensional pixel space. Not all pixel configurations correspond to realistic faces; the \emph{face manifold} represents the subset of pixel space that depicts plausible human faces. Perturbations must respect manifold constraints to ensure counterfactuals are meaningful.

\textbf{Embedding Space.} Face verification models map images to fixed-dimensional vectors (embeddings) in a learned metric space where cosine similarity or Euclidean distance reflects perceptual similarity. Typical embeddings are 512-dimensional L2-normalized vectors. Verification decisions are based on embedding distances rather than direct pixel comparisons.

\subsection{Notation}

The following mathematical notation is used throughout:

\begin{itemize}
\item $\mathbf{x}, \mathbf{x}' \in \mathbb{R}^{H \times W \times 3}$: Face images (height $H$, width $W$, RGB channels)
\item $f: \mathbb{R}^{H \times W \times 3} \to \mathbb{R}^d$: Face verification model (encoder) producing $d$-dimensional embeddings
\item $\mathbf{z} = f(\mathbf{x}) \in \mathbb{R}^d$: Face embedding, typically with $d=512$ and $\|\mathbf{z}\|_2 = 1$
\item $s(\mathbf{x}, \mathbf{x}') = \langle f(\mathbf{x}), f(\mathbf{x}') \rangle$: Verification score (cosine similarity) in $[-1, 1]$
\item $A(\mathbf{x}, \mathbf{x}') \in \mathbb{R}^{H \times W}$: Attribution map assigning importance to each spatial location
\item $\mathbf{x}_\delta$: Perturbed (counterfactual) version of $\mathbf{x}$ with perturbation strength $\delta$
\item $\Delta s = s(\mathbf{x}_\delta, \mathbf{x}') - s(\mathbf{x}, \mathbf{x}')$: Predicted score change under perturbation
\item $\mathcal{M} \subset \mathbb{R}^{H \times W \times 3}$: Face manifold (set of plausible face images)
\item $\mathcal{D} = \{(\mathbf{x}_i, \mathbf{x}'_i, y_i)\}_{i=1}^N$: Dataset of image pairs with verification labels $y_i \in \{0,1\}$
\end{itemize}

Additional notation is introduced locally in technical chapters as needed. Standard conventions from machine learning and computer vision literature apply throughout.

\subsection{Terminology Note}

Throughout this dissertation, we use the following terms consistently:
\begin{itemize}
\item \textbf{``Attribution method''} and \textbf{``explanation method''} interchangeably (both refer to XAI techniques)
\item \textbf{``Faithfulness''} as the preferred term (though ``fidelity'' appears in some cited works)
\item \textbf{``Feature''} context-dependent: interpretable image regions (superpixels or spatial blocks from Grad-CAM, SHAP, LIME) in XAI context, versus low-level CNN features in model internals context
\item \textbf{``Hypersphere embedding''} refers to L2-normalized vectors lying on the surface of a high-dimensional unit sphere, where similarity is measured via angular distance (geodesic distance = arccos of cosine similarity)
\end{itemize}

See Section \ref{sec:terminology} (Terminology and Notation) for complete definitions.
