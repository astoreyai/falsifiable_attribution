% Abstract for Falsifiable Attribution Dissertation
% This file is included in main.tex via \abstract{}

Face verification systems powered by deep neural networks have been increasingly deployed in high-stakes contexts, from law enforcement and border security to financial services and access control. Despite achieving remarkable accuracy on benchmark datasets, these systems suffer from documented demographic disparities and fundamental opacity in decision-making. Multiple wrongful arrests have occurred when facial recognition systems produced incorrect matches that law enforcement personnel could not meaningfully evaluate due to the lack of validated explanations. This accountability gap poses severe challenges for individual rights and civil liberties.

This dissertation addresses the fundamental problem that current explainable AI (XAI) methods lack falsifiability: there exists no principled method to validate whether generated explanations faithfully represent the model's decision-making process or merely constitute plausible post-hoc rationalizations. Existing evaluation approaches rely on proxy metrics with known theoretical flaws, cannot distinguish between competing explanations, and fail to detect systematic misattribution of causality. Legal and forensic contexts demand stronger epistemic guarantees than current methods provide.

This research develops a counterfactual falsifiability framework specifically designed for face verification systems operating in embedding spaces. The approach treats attribution faithfulness as a testable hypothesis: if an attribution method correctly identifies important features, then systematically perturbing those features in controlled ways should produce predictable changes in verification scores. The framework establishes formal falsifiability criteria, derives computational and geometric bounds on achievable faithfulness, and provides systematic evaluation protocols for existing attribution methods.

Through rigorous empirical evaluation on benchmark datasets (VGGFace2-HQ, LFW, CFP-FP, AgeDB-30), this work reveals that no existing attribution method (Grad-CAM, SHAP, Integrated Gradients, LIME) consistently produces falsifiable explanations across all test conditions. Performance varies significantly by face pose, demographic group, and verification score range. The research identifies specific failure modes and establishes evidence-based deployment guidelines for forensic and legal contexts.

The contributions include: (1)~the first formal falsifiability framework for pairwise verification XAI, (2)~computational and geometric bounds on attribution faithfulness in hypersphere embedding spaces, (3)~plausibility-preserving perturbation strategies for face manifolds, (4)~systematic evaluation revealing when and why current methods fail, (5)~benchmark suite with ground truth test cases, (6)~open-source evaluation framework for reproducible XAI assessment, and (7)~deployment thresholds meeting regulatory requirements (EU AI Act, GDPR Article 22, Daubert standard for scientific evidence).

This research advances explainable AI by establishing objective, falsifiable evaluation criteria that can distinguish faithful explanations from plausible confabulations---a prerequisite for the responsible deployment of face verification systems in contexts where incorrect decisions carry profound consequences for individual rights and freedoms.
